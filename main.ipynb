{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from utils import *\n",
    "from model import *\n",
    "from dataset import *\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "model_file = \"./data/kowiki.model\" \n",
    "\n",
    "# SentencePiece 모델 로드\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1941년 이우가 배속된 소속은?</td>\n",
       "      <td>조선군사령부</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1787년경 프랑스에서 거두고 있던 소득세는 무엇일까?</td>\n",
       "      <td>벵티엠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>늑대 보호 조치와 효과적 법 집행으로 적당한 늑대 개체수를 유지하고 있는 국가는?</td>\n",
       "      <td>이스라엘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>독도가 한국영토로 표기된 사례는 세계지도 3380건 중에 몇 건인가?</td>\n",
       "      <td>49건</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>중력과 관계된 낙차에 의해 움직여지는 수차를 무엇이라 부르는가?</td>\n",
       "      <td>중력수차</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>나랑 놀아줘</td>\n",
       "      <td>같이 놀아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>친구들한테 인기 얻으려면</td>\n",
       "      <td>성격이 좋으면 인기가 있을 거예요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>시민단체는 정부가 기초연금에 대해 어떤 원리를 경직되게 적용한다고 보았는가?</td>\n",
       "      <td>보충성의 원리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>한국독립당이 창당될 때  안창호 외 창당 발기인은 몇 명이었는가?</td>\n",
       "      <td>28명</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>노무현이 5공 청문회에서 명패를 던졌던 사람은?</td>\n",
       "      <td>전두환</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Q                    A\n",
       "0                             1941년 이우가 배속된 소속은?               조선군사령부\n",
       "1                 1787년경 프랑스에서 거두고 있던 소득세는 무엇일까?                  벵티엠\n",
       "2  늑대 보호 조치와 효과적 법 집행으로 적당한 늑대 개체수를 유지하고 있는 국가는?                 이스라엘\n",
       "3         독도가 한국영토로 표기된 사례는 세계지도 3380건 중에 몇 건인가?                  49건\n",
       "4            중력과 관계된 낙차에 의해 움직여지는 수차를 무엇이라 부르는가?                 중력수차\n",
       "5                                         나랑 놀아줘              같이 놀아요.\n",
       "6                                  친구들한테 인기 얻으려면  성격이 좋으면 인기가 있을 거예요.\n",
       "7     시민단체는 정부가 기초연금에 대해 어떤 원리를 경직되게 적용한다고 보았는가?              보충성의 원리\n",
       "8           한국독립당이 창당될 때  안창호 외 창당 발기인은 몇 명이었는가?                  28명\n",
       "9                     노무현이 5공 청문회에서 명패를 던졌던 사람은?                  전두환"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset_name = 'ChatbotData_KorQuAD'\n",
    "csv_path = './data/ChatbotData_KorQuAD.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q seq_max_len: 78\n",
      "A seq_max_len: 78\n",
      "Original Text: 1941년 이우가 배속된 소속은?\n",
      "Tokens: ['▁194', '1', '년', '▁이', '우', '가', '▁배', '속', '된', '▁소속', '은', '?']\n",
      "IDs: [429, 3597, 3616, 8, 3679, 3599, 179, 3763, 3703, 765, 3604, 4245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Original Text: 1787년경 프랑스에서 거두고 있던 소득세는 무엇일까?\n",
      "Tokens: ['▁17', '87', '년', '경', '▁프랑스', '에서', '▁거두', '고', '▁있던', '▁소', '득', '세는', '▁무', '엇', '일', '까', '?']\n",
      "IDs: [381, 3209, 3616, 3673, 542, 10, 1987, 3600, 804, 68, 4054, 1561, 108, 4491, 3620, 3794, 4245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Original Text: 늑대 보호 조치와 효과적 법 집행으로 적당한 늑대 개체수를 유지하고 있는 국가는?\n",
      "Tokens: ['▁', '늑', '대', '▁보호', '▁조', '치', '와', '▁효과', '적', '▁법', '▁집', '행', '으로', '▁적', '당한', '▁', '늑', '대', '▁개', '체', '수를', '▁유지', '하고', '▁있는', '▁국가', '는', '?']\n",
      "IDs: [3587, 4701, 3612, 1627, 53, 3689, 3655, 1902, 3647, 373, 313, 3742, 9, 238, 3210, 3587, 4701, 3612, 72, 3740, 930, 1137, 47, 89, 352, 3593, 4245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Original Text: 독도가 한국영토로 표기된 사례는 세계지도 3380건 중에 몇 건인가?\n",
      "Tokens: ['▁독', '도가', '▁한국', '영', '토', '로', '▁표기', '된', '▁사', '례', '는', '▁세계', '지도', '▁3', '3', '80', '건', '▁중에', '▁몇', '▁건', '인', '가', '?']\n",
      "IDs: [193, 881, 323, 3707, 3773, 3594, 2512, 3703, 15, 4047, 3593, 319, 2848, 49, 3643, 1695, 3795, 2007, 856, 218, 3619, 3599, 4245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Original Text: 중력과 관계된 낙차에 의해 움직여지는 수차를 무엇이라 부르는가?\n",
      "Tokens: ['▁중', '력과', '▁관계', '된', '▁낙', '차', '에', '▁의해', '▁움직', '여', '지는', '▁수', '차를', '▁무', '엇', '이라', '▁부', '르는', '가', '?']\n",
      "IDs: [36, 3099, 703, 3703, 1581, 3741, 3591, 356, 2003, 3645, 369, 19, 3440, 108, 4491, 120, 51, 775, 3599, 4245, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#시퀀스 max 길이 찾기\n",
    "seq_max_len = 0\n",
    "for line in list(df['Q'].values):\n",
    "    leng = len(sp.encode_as_ids(line))\n",
    "    if seq_max_len < leng:\n",
    "        seq_max_len = leng\n",
    "print(\"Q seq_max_len:\", seq_max_len)\n",
    "\n",
    "\n",
    "for line in list(df['A'].values):\n",
    "    leng = len(sp.encode_as_ids(line))\n",
    "    if seq_max_len < leng:\n",
    "        seq_max_len = leng\n",
    "print(\"A seq_max_len:\", seq_max_len)\n",
    "\n",
    "#학습데이터 Vocab 적용해보기\n",
    "ids_stack = []\n",
    "for line in list(df['Q'][:5].values):\n",
    "    pieces = sp.encode_as_pieces(line)\n",
    "    ids = sp.encode_as_ids(line)\n",
    "    ids += (seq_max_len-len(ids))*[0]\n",
    "    ids_stack.append(ids)\n",
    "    print(\"Original Text:\", line)\n",
    "    print(\"Tokens:\", pieces)\n",
    "    print(\"IDs:\", ids)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length: 65007\n",
      "val_dataset length: 7223\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChatBotDataset(csv_path, seq_max_len)\n",
    "val_dataset = ChatBotDataset(csv_path, seq_max_len, train=False)\n",
    "\n",
    "print(\"train_dataset length:\", len(train_dataset))\n",
    "print(\"val_dataset length:\", len(val_dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled_dot_Attention 계산 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답: tensor([[[1.1807, 2.0000, 2.8193],\n",
      "         [2.8193, 2.0000, 1.1807],\n",
      "         [1.1807, 2.0000, 2.8193]],\n",
      "\n",
      "        [[3.0000, 2.0000, 2.0000],\n",
      "         [3.0000, 2.0000, 2.0000],\n",
      "         [3.0000, 2.0000, 2.0000]]])\n"
     ]
    }
   ],
   "source": [
    "n_dim = 3\n",
    "q = torch.tensor([[[1,2,3],\n",
    "                   [3,2,1],\n",
    "                   [4,5,6]],\n",
    "                  \n",
    "                  [[3,2,2],\n",
    "                   [1,1,1],\n",
    "                   [5,2,4]]], dtype=torch.float32)\n",
    "k = q.transpose(-1,-2)\n",
    "token_ids = torch.tensor([[1,1,0], [1,0,0]])\n",
    "\n",
    "\n",
    "scaled_attention = torch.matmul(q,k) / np.sqrt(n_dim)\n",
    "masked_attention =  making_padding_mask(scaled_attention, token_ids)\n",
    "attention_score = torch.softmax(masked_attention, dim=-1)\n",
    "output = torch.matmul(attention_score, q)\n",
    "print(\"정답:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1136, -0.9815,  2.9034],\n",
       "         [ 0.0749, -1.1777,  2.9638],\n",
       "         [ 0.2185, -1.3270,  3.0097]],\n",
       "\n",
       "        [[-0.0239, -1.0855,  3.2401],\n",
       "         [-0.0239, -1.0855,  3.2401],\n",
       "         [-0.0239, -1.0855,  3.2401]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#확인할 때 q,k,v의 가중치는 제외하고 확인해볼 것\n",
    "model = ScaledDotProductAttention(3)\n",
    "model(q,q,q,token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "#model hyperparameter\n",
    "model_name = 'transformer_chatbot'\n",
    "n_seq = seq_max_len\n",
    "n_vocab = sp.vocab_size()\n",
    "# 논문의 절반으로 setting\n",
    "n_dim = 256\n",
    "n_head = 4\n",
    "n_layer = 3\n",
    "\n",
    "# dataloader hyperparameter\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#train, Loss, Optimizer hyperparameter\n",
    "epochs = 100\n",
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(n_seq, n_vocab, n_dim, n_head, n_layer, device=device).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss_fn, optimizer, save_path, eval_step=1):\n",
    "    \n",
    "    wandb.init(\n",
    "    project='Transformer_ChatBot',\n",
    "    name = f'{model_name}_{dataset_name}',\n",
    "    config={\n",
    "        \"architecture\": model_name,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"batch_size\" : batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"n_seq\": n_seq,\n",
    "        \"n_vocab\": n_vocab,\n",
    "        \"n_dim\": n_dim,\n",
    "        \"n_head\": n_head,\n",
    "        \"n_layer\": n_layer,\n",
    "        \"loss_fn\": \"CrossEntropyLoss\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        })\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "    best_val_loss = np.inf\n",
    "    # BOS 토큰 ID를 가져옵니다.\n",
    "    bos_token_id = sp.bos_id()\n",
    "    for epoch in tqdm(range(epochs), ascii=True, desc=\"epoch\"):\n",
    "        print()\n",
    "        print(f\"********** epoch{epoch+1} train start **********\")\n",
    "        train_loss = 0\n",
    "        for idx, (q, a) in enumerate(train_dataloader):\n",
    "            q, a = q.to(device), a.to(device)\n",
    "            # a를 bos_a로 변경 (오른쪽 시프트)\n",
    "            bos_a = shift_right(a, bos_token_id)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(q, bos_a)\n",
    "            loss = loss_fn(pred.view(-1, pred.size(-1)), a.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if idx % (len(train_dataloader)//3) == 0:\n",
    "                print(\"epoch: {}, loss: {}\".format(epoch+1, loss.item()))\n",
    "        train_metrics = {'train_loss': train_loss/len(train_dataloader)}\n",
    "        \n",
    "        print(\"epoch: {}, train_loss: {}\".format(epoch+1, train_loss/len(train_dataloader)))\n",
    "        print(\"********** train end **********\")\n",
    "        wandb.log(train_metrics, step=epoch)\n",
    "        print()\n",
    "        if epoch % eval_step == 0:\n",
    "            with torch.no_grad():\n",
    "                print(\"********** eval start **********\")\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                for idx, (q, a) in enumerate(val_dataloader):\n",
    "                    q, a = q.to(device), a.to(device)\n",
    "                    pred = model(q, a)\n",
    "                    loss = loss_fn(pred.view(-1, pred.size(-1)), a.view(-1))\n",
    "                    val_loss += loss.item()\n",
    "                \n",
    "                if best_val_loss > val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    print()\n",
    "                    print(\"****best model saved****\")\n",
    "                    print()\n",
    "                val_metrics = {'val_loss': val_loss/len(val_dataloader)}\n",
    "                print(\"epoch: {}, val_loss: {}\".format(epoch+1, val_loss/len(val_dataloader)))\n",
    "                \n",
    "                #문장 만들기\n",
    "                indx = np.random.randint(0, len(q))#0~len(val_dataset) 사이의 숫자 랜덤으로 뽑기\n",
    "                max_arg = torch.argmax(pred, dim=-1)\n",
    "                pred_sentc = sp.DecodeIds(max_arg[indx].tolist())\n",
    "                label_sentc = sp.DecodeIds(a[indx].tolist())\n",
    "                q_sentc = sp.DecodeIds(q[indx].tolist())\n",
    "                print(\"Q:\", q_sentc)\n",
    "                print(\"Pred:\", pred_sentc)\n",
    "                print(\"Label:\", label_sentc)\n",
    "                wandb.log(val_metrics, step=epoch)\n",
    "                print(\"********** eval end **********\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdwkim8155\u001b[0m (\u001b[33mboostcamp-oif\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/Woo/Desktop/kaist/1-1/NLP-Summer/Transformer 구현/wandb/run-20240708_001012-h1k9hz1c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/boostcamp-oif/Transformer_ChatBot/runs/h1k9hz1c' target=\"_blank\">transformer_chatbot_ChatbotData_KorQuAD</a></strong> to <a href='https://wandb.ai/boostcamp-oif/Transformer_ChatBot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/boostcamp-oif/Transformer_ChatBot' target=\"_blank\">https://wandb.ai/boostcamp-oif/Transformer_ChatBot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/boostcamp-oif/Transformer_ChatBot/runs/h1k9hz1c' target=\"_blank\">https://wandb.ai/boostcamp-oif/Transformer_ChatBot/runs/h1k9hz1c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********epoch1 train start**********\n",
      "epoch: 1, loss: 9.135636329650879\n",
      "epoch: 1, loss: 0.882185697555542\n",
      "epoch: 1, loss: 0.8259548544883728\n",
      "epoch: 1, loss: 0.6149036884307861\n",
      "epoch: 0, train_loss: 1.1399515186707805\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   1%|1         | 1/100 [03:07<5:09:42, 187.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****best model saved****\n",
      "\n",
      "epoch: 1, val_loss: 0.6459396887669521\n",
      "Q: J. K. 롤링이 책을 집필하기 위해 자주 이용했던 곳은?\n",
      "Pred: \n",
      "Label: 카페\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch2 train start**********\n",
      "epoch: 2, loss: 0.6310096383094788\n",
      "epoch: 2, loss: 0.5644517540931702\n",
      "epoch: 2, loss: 0.5706245303153992\n",
      "epoch: 2, loss: 0.4819130599498749\n",
      "epoch: 1, train_loss: 0.5767584623783593\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   2%|2         | 2/100 [06:16<5:07:32, 188.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****best model saved****\n",
      "\n",
      "epoch: 2, val_loss: 0.5387054861119364\n",
      "Q: 별에서온 그대에서 안재현이 나온 역할은?\n",
      "Pred: \n",
      "Label: 천윤재 역할\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch3 train start**********\n",
      "epoch: 3, loss: 0.5028736591339111\n",
      "epoch: 3, loss: 0.46147820353507996\n",
      "epoch: 3, loss: 0.5061978101730347\n",
      "epoch: 3, loss: 0.4949367642402649\n",
      "epoch: 2, train_loss: 0.5016547492228625\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   3%|3         | 3/100 [09:23<5:03:31, 187.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****best model saved****\n",
      "\n",
      "epoch: 3, val_loss: 0.5184027927111735\n",
      "Q: IRB는 최소 몇명 이상으로 구성되어야 하는가?\n",
      "Pred: \n",
      "Label: 5명 이상\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch4 train start**********\n",
      "epoch: 4, loss: 0.5239384770393372\n",
      "epoch: 4, loss: 0.49657753109931946\n",
      "epoch: 4, loss: 0.3507120609283447\n",
      "epoch: 4, loss: 0.41790154576301575\n",
      "epoch: 3, train_loss: 0.46653400666601075\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   4%|4         | 4/100 [12:29<4:59:01, 186.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, val_loss: 0.5298156168608539\n",
      "Q: 별에서온 그대에서 안재현이 나온 역할은?\n",
      "Pred: \n",
      "Label: 천윤재 역할\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch5 train start**********\n",
      "epoch: 5, loss: 0.47072434425354004\n",
      "epoch: 5, loss: 0.4913058280944824\n",
      "epoch: 5, loss: 0.4560984969139099\n",
      "epoch: 5, loss: 0.4070553779602051\n",
      "epoch: 4, train_loss: 0.43920606761936126\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   5%|5         | 5/100 [15:35<4:55:51, 186.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, val_loss: 0.5534577583317208\n",
      "Q: 민주노동당 당원 가족들을 불법사찰한 사실을 폭로한 날짜는?\n",
      "Pred: \n",
      "Label: 8월 17일\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch6 train start**********\n",
      "epoch: 6, loss: 0.48735663294792175\n",
      "epoch: 6, loss: 0.41468673944473267\n",
      "epoch: 6, loss: 0.442660391330719\n",
      "epoch: 6, loss: 0.4618214964866638\n",
      "epoch: 5, train_loss: 0.41947527614048147\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   6%|6         | 6/100 [18:41<4:52:13, 186.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, val_loss: 0.5777770615784468\n",
      "Q: 제천 스포츠센터 건축시 외장재를 어떤 재질로 건축하였는가?\n",
      "Pred: \n",
      "Label: 드라이비트\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch7 train start**********\n",
      "epoch: 7, loss: 0.4341537654399872\n",
      "epoch: 7, loss: 0.45753028988838196\n",
      "epoch: 7, loss: 0.4517773389816284\n",
      "epoch: 7, loss: 0.3548433482646942\n",
      "epoch: 6, train_loss: 0.40478366281925227\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   7%|7         | 7/100 [21:47<4:48:40, 186.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, val_loss: 0.5992654430127777\n",
      "Q: 오스트레일리아까치는 얼마나 높은 음을 낼 수 있나?\n",
      "Pred: 일\n",
      "Label: 4 옥타브 이상\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch8 train start**********\n",
      "epoch: 8, loss: 0.34922415018081665\n",
      "epoch: 8, loss: 0.40957313776016235\n",
      "epoch: 8, loss: 0.3464091420173645\n",
      "epoch: 8, loss: 0.4095548093318939\n",
      "epoch: 7, train_loss: 0.3932300850338354\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   8%|8         | 8/100 [24:53<4:45:21, 186.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, val_loss: 0.6191616443406164\n",
      "Q: 서독에서 출판된 라살로 고발된 슈테판 하임이 받은 선고는?\n",
      "Pred: 스\n",
      "Label: 벌금형\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch9 train start**********\n",
      "epoch: 9, loss: 0.3891252279281616\n",
      "epoch: 9, loss: 0.47059258818626404\n",
      "epoch: 9, loss: 0.41786864399909973\n",
      "epoch: 9, loss: 0.3897298276424408\n",
      "epoch: 8, train_loss: 0.38339548889459585\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   9%|9         | 9/100 [27:58<4:41:58, 185.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, val_loss: 0.6390705135016315\n",
      "Q: 야마우치 가즈토요가 이에야스에게 제공하기로 하여 환심을 산 성의 이름은?\n",
      "Pred: \n",
      "Label: 가케가와 성\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch10 train start**********\n",
      "epoch: 10, loss: 0.3929477632045746\n",
      "epoch: 10, loss: 0.3707222640514374\n",
      "epoch: 10, loss: 0.34809935092926025\n",
      "epoch: 10, loss: 0.3578353822231293\n",
      "epoch: 9, train_loss: 0.3744176293569287\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|#         | 10/100 [31:04<4:38:38, 185.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, val_loss: 0.6626675070914547\n",
      "Q: 보잉사가 가진 업무 철학은 무엇인가?\n",
      "Pred: 리니\n",
      "Label: 장벽 제거 철학\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch11 train start**********\n",
      "epoch: 11, loss: 0.3556901514530182\n",
      "epoch: 11, loss: 0.3631409704685211\n",
      "epoch: 11, loss: 0.3479026257991791\n",
      "epoch: 11, loss: 0.36990073323249817\n",
      "epoch: 10, train_loss: 0.3658619427716169\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  11%|#1        | 11/100 [34:09<4:35:25, 185.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, val_loss: 0.6959032674806308\n",
      "Q: 유전형질의 적응도가 더욱 증가하는 경우의 선택은?\n",
      "Pred: 리\n",
      "Label: 안정성 선택\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch12 train start**********\n",
      "epoch: 12, loss: 0.3858884274959564\n",
      "epoch: 12, loss: 0.3681367039680481\n",
      "epoch: 12, loss: 0.4007795751094818\n",
      "epoch: 12, loss: 0.35060515999794006\n",
      "epoch: 11, train_loss: 0.3582985148889812\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  12%|#2        | 12/100 [37:15<4:32:21, 185.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, val_loss: 0.7192110577515797\n",
      "Q: 이어폰 사야지\n",
      "Pred: 나\n",
      "Label: 잘 골라보세요.\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch13 train start**********\n",
      "epoch: 13, loss: 0.45323240756988525\n",
      "epoch: 13, loss: 0.3536745309829712\n",
      "epoch: 13, loss: 0.33765509724617004\n",
      "epoch: 13, loss: 0.3931470811367035\n",
      "epoch: 12, train_loss: 0.3515262484227813\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  13%|#3        | 13/100 [40:21<4:29:18, 185.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, val_loss: 0.7483261428048126\n",
      "Q: 오버워치를 출시한 게임 회사의 이름은?\n",
      "Pred: 리\n",
      "Label: 블리자드\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch14 train start**********\n",
      "epoch: 14, loss: 0.33954039216041565\n",
      "epoch: 14, loss: 0.3155936300754547\n",
      "epoch: 14, loss: 0.34418460726737976\n",
      "epoch: 14, loss: 0.33991003036499023\n",
      "epoch: 13, train_loss: 0.34525459550145104\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  14%|#4        | 14/100 [43:26<4:26:08, 185.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, val_loss: 0.7743430185107003\n",
      "Q: 이어폰 사야지\n",
      "Pred: 나\n",
      "Label: 잘 골라보세요.\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch15 train start**********\n",
      "epoch: 15, loss: 0.35961607098579407\n",
      "epoch: 15, loss: 0.35164675116539\n",
      "epoch: 15, loss: 0.33552640676498413\n",
      "epoch: 15, loss: 0.28418052196502686\n",
      "epoch: 14, train_loss: 0.339341731316696\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  15%|#5        | 15/100 [47:01<4:35:33, 194.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, val_loss: 0.7851275063194005\n",
      "Q: 오스트레일리아까치는 얼마나 높은 음을 낼 수 있나?\n",
      "Pred: 니이\n",
      "Label: 4 옥타브 이상\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch16 train start**********\n",
      "epoch: 16, loss: 0.3176496922969818\n",
      "epoch: 16, loss: 0.3642856478691101\n",
      "epoch: 16, loss: 0.3594782054424286\n",
      "epoch: 16, loss: 0.32227662205696106\n",
      "epoch: 15, train_loss: 0.3336987718939781\n",
      "**********train end**********\n",
      "\n",
      "**********eval start**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  16%|#6        | 16/100 [1:12:08<13:45:27, 589.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, val_loss: 0.7996084136245525\n",
      "Q: 말제르브가 정치에 개입하기 시작한 해는?\n",
      "Pred: 권\n",
      "Label: 1771년\n",
      "**********eval end**********\n",
      "\n",
      "**********epoch17 train start**********\n",
      "epoch: 17, loss: 0.26528438925743103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  16%|#6        | 16/100 [1:12:23<6:20:01, 271.44s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./weight/best_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loss_fn, optimizer, save_path, eval_step)\u001b[0m\n\u001b[1;32m     36\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(q, bos_a)\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, pred\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), a\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp-summer/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp-summer/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp-summer/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = f'./weight/best_{model_name}.pt'\n",
    "train(loss_fn, optimizer,save_path, eval_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: 내일 뭐해?\n",
      "출력 문장: 년\n"
     ]
    }
   ],
   "source": [
    "#저장된 best 가중치 불러오기\n",
    "save_path = f'./weight/best_{model_name}.pt'\n",
    "state_dict = torch.load(save_path)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "prompt = \"내일 뭐해?\"\n",
    "output = generate_sentc(model, sp, n_seq, prompt, device)\n",
    "print(\"입력 문장:\", prompt)\n",
    "print(\"출력 문장:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
